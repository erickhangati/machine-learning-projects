# ğŸ§  MNIST Digit Classification with Neural Networks (NumPy Only)

This project demonstrates how to build and train a simple feedforward neural network from scratch using **NumPy** to classify handwritten digits from the **MNIST dataset**.

Unlike traditional projects that rely on libraries like TensorFlow or PyTorch, this one walks through every step of the neural network manually â€” helping deepen intuition about how neural nets work under the hood.

---

## ğŸ“Œ What This Project Covers

- ğŸ“¦ Data loading and preprocessing (flattening, one-hot encoding, normalization)
- ğŸ”¢ Weight and bias initialization
- ğŸ§  Forward propagation (ReLU + Softmax)
- ğŸ¯ Categorical cross-entropy loss computation
- ğŸ” Backpropagation and gradient descent training loop
- ğŸ“‰ Loss visualization over time
- âœ… Accuracy evaluation on test set
- ğŸ–¼ï¸ Visualization of model predictions on real images

---

## ğŸš€ Results

- **Final Test Accuracy:** ~97.5%
- Achieved using only:
  - NumPy
  - No external machine learning libraries
  - One hidden layer with 128 neurons
  - ReLU and Softmax activations

---

## ğŸ“‚ File Structure

```
deep-learning/
â”œâ”€â”€ neural-network.ipynb
â””â”€â”€ README.md (this file)
```

---

## ğŸ“ Why This Project Matters

This project shows a deep understanding of how neural networks work by avoiding high-level ML tools and implementing everything manually. Itâ€™s a great learning experience â€” and an excellent portfolio piece for demonstrating:

- Model intuition
- Math-to-code translation
- Debugging from the ground up
- Real-world dataset experience (MNIST)

---

## ğŸ§  Dataset Info

- **Source:** [MNIST Digits Dataset](http://yann.lecun.com/exdb/mnist/)
- **Classes:** 10 (digits 0 to 9)
- **Shape:** 60,000 training images, 10,000 test images (28Ã—28 grayscale)

---

## âœ… Getting Started

To run the notebook:

1. Clone the repository
2. Install NumPy and Matplotlib
3. Launch `mnist-digit-classification.ipynb` in Jupyter
4. Run all cells to train and test the model
