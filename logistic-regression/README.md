# ğŸ“§ Spam Email Classification with Logistic Regression (From Scratch Using NumPy)

This project demonstrates how to build a **logistic regression model from scratch using NumPy** to classify emails as spam (1) or ham (0), based on real-world datasets.

We apply classic machine learning techniques to text data without relying on machine learning libraries â€” showcasing a full understanding of the math and workflow behind logistic regression.

---

## ğŸ§¾ Dataset

- Combined from:
  - **2007 TREC Public Spam Corpus**
  - **Enron-Spam Dataset**
- Each record contains:
  - `text`: Raw email content
  - `label`: 1 for spam, 0 for ham

Source: [Kaggle - Spam Email Classification Dataset](https://www.kaggle.com/datasets/purusinghvi/email-spam-classification-dataset)

---

## ğŸ§  Project Highlights

- ğŸ§¹ Cleaned and preprocessed 10,000+ raw email texts
- ğŸ§® Transformed text to numerical vectors using Bag-of-Words (`CountVectorizer`)
- ğŸ” Implemented **logistic regression manually** (NumPy only)
- ğŸ“‰ Computed binary cross-entropy loss and gradient updates
- ğŸ”’ Added **L2 regularization** to improve generalization
- ğŸ“Š Evaluated accuracy, confusion matrix, and classification report
- ğŸ§ª Visualized data with PCA for interpretation

---

## ğŸ“ˆ Model Performance

| Metric     | Value (Example Run) |
|------------|---------------------|
| Accuracy   | ~93% (after tuning) |
| Model      | Logistic Regression |
| Features   | Top 3,000 words     |
| Regularization | L2 (`lambda=10.0`)  |

---

## ğŸ“‚ Folder Structure

```
logistic-regression/
â”œâ”€â”€ combined-data.csv 
â”œâ”€â”€ email-spam-classification.ipynb # Jupyter notebook (main project)
â””â”€â”€ README.md # This file
```
---

## ğŸš€ How to Run

1. Download the dataset from Kaggle and place it as `combined-data.csv` inside the `logistic-regression/` folder.
2. Install required libraries:

```bash
pip install numpy pandas matplotlib scikit-learn
```

---

## âœï¸ What I Learned
- Implementing ML algorithms from first principles.
- Applying NLP preprocessing on real-world text.
- How logistic regression makes probabilistic predictions.
- The role of regularization in preventing overfitting.