# ğŸ§¬ Breast Cancer Detection with SVM (Built from Scratch Using NumPy)

This project implements a **Support Vector Machine (SVM)** classifier **from scratch using NumPy** to predict whether a tumor is **benign or malignant** based on medical diagnostic features.  

It demonstrates the **mathematical and algorithmic foundation of SVMs** â€” from Hinge Loss computation to Gradient Descent optimization â€” without relying on pre-built machine learning libraries like scikit-learn.

---

## ğŸ§¾ Dataset

- **Source:** Built-in **Breast Cancer Wisconsin (Diagnostic) dataset** from the [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) library  
- The dataset originates from the **UCI Machine Learning Repository** but is conveniently accessible through scikit-learn.  
- Each record represents computed features from a breast massâ€™s digital image.  
- The dataset contains **569 samples** and **30 numerical features** describing various **cell nuclei characteristics** derived from digitized images of fine needle aspirates (FNA) of breast masses.

**Key features include:**
- `radius_mean` â€“ Mean radius of cell nuclei  
- `texture_mean` â€“ Standard deviation of gray-scale values  
- `perimeter_mean` â€“ Mean size of the cell perimeter  
- `area_mean` â€“ Mean area of the cell nuclei  
- `smoothness_mean` â€“ Variation in radius lengths  
- `compactness_mean`, `concavity_mean`, `symmetry_mean`, etc.  
- `diagnosis` â€“ Target variable: `M = Malignant`, `B = Benign`

---

## ğŸ§  Project Highlights

- ğŸ” Loaded and cleaned the Breast Cancer dataset from scikit-learn  
- âš–ï¸ Standardized feature values for optimal model convergence  
- ğŸ§® Implemented **Linear SVM from scratch using NumPy**:
  - Custom **Hinge Loss** with **L2 regularization**  
  - Manual **Gradient Descent** optimization  
  - Weight and bias updates computed step-by-step  
  - Implemented convergence check to stop when loss stabilizes  
- ğŸ“Š Visualized **loss convergence** during training  
- ğŸ¯ Achieved **98% accuracy** on the test dataset  
- âœ… Compared custom implementation results with **scikit-learnâ€™s SVM** for validation

---

## ğŸ“ˆ Model Performance (Example Results)

| Metric     | Score |
|-------------|--------|
| Accuracy    | 0.982 |
| Precision (Malignant) | 0.97 |
| Recall (Malignant) | 0.96 |
| F1-Score (Malignant) | 0.96 |

<br>

**Confusion Matrix**

```
[[106 2]
 [3 59]]
```


- 106 benign tumors correctly classified  
- 59 malignant tumors correctly classified  
- Only 5 total misclassifications out of 170 test samples  

The model demonstrates strong generalization with near-perfect separation between the two classes.

---

## ğŸ“‚ Folder Structure

```
/svm-with-numpy/
    â”œâ”€â”€ breast-cancer-detection.ipynb # Jupyter Notebook
    â””â”€â”€ README.md # This file
```

---

## ğŸš€ How to Run

1. Clone the repository:
   ```bash
   git clone https://github.com/erickhangati/machine-learning-projects.git
   cd machine-learning-projects/SVM/svm-with-numpy
   ```
2. Install dependencies:
   ```bash
   pip install pandas numpy matplotlib seaborn scikit-learn
   ```
3. Install dependencies:
   ```bash
   jupyter notebook knn-with-numpy.ipynb
   ```
   
## âœï¸ What I Learned

- How SVMs maximize the decision boundary margin to achieve strong generalization.
- How to implement Hinge Loss and Gradient Descent manually using only NumPy.
- The role of regularization (Î») in balancing margin width and misclassification.
- The importance of feature scaling for distance-based optimization.
- How to compare and validate custom models against scikit-learn implementations.

## ğŸ“˜ Blog Article

A full, beginner-friendly walkthrough of this project is available on my blog:
[Build a Support Vector Machine From Scratch using Numpy](https://erickhangati.com/build-a-support-vector-machine-from-using-numpy/)