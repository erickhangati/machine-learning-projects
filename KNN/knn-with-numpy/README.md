# ğŸ©º Diabetes Prediction with KNN (Built from Scratch Using NumPy)

This project builds a **K-Nearest Neighbors (KNN)** classifier **from scratch using NumPy** to predict whether a patient has diabetes based on key medical features.  

It demonstrates how KNN works internally, from distance calculation to majority voting, without using pre-built machine learning libraries.

---

## ğŸ§¾ Dataset

- **Source:** [Kaggle - Diabetes Dataset](https://www.kaggle.com/datasets/nanditapore/healthcare-diabetes)
- Each record contains medical measurements such as:
  - `Pregnancies` â€“ Number of times pregnant  
  - `Glucose` â€“ Plasma glucose concentration  
  - `BloodPressure` â€“ Diastolic blood pressure  
  - `SkinThickness` â€“ Triceps skin fold thickness  
  - `Insulin` â€“ Serum insulin level  
  - `BMI` â€“ Body Mass Index  
  - `DiabetesPedigreeFunction` â€“ Genetic risk measure  
  - `Age` â€“ Patient age in years  
  - `Outcome` â€“ 1 = Diabetic, 0 = Non-diabetic

---

## ğŸ§  Project Highlights

- ğŸ§¹ Cleaned and prepared 2,700+ patient records  
- ğŸ” Replaced biologically implausible zeros with missing values and filled them using the median  
- ğŸ“Š Explored feature distributions and relationships with the target variable  
- ğŸ“ˆ Implemented **KNN from scratch using only NumPy**:
  - Custom Euclidean distance function  
  - Manual distance computation and majority voting  
  - Feature scaling for distance fairness  
- ğŸ§® Achieved **83.5% accuracy** on the test data  
- ğŸ“‹ Evaluated performance using **confusion matrix**, **precision**, **recall**, and **F1-score**

---

## ğŸ“ˆ Model Performance (Example Results)

| Metric     | Score |
|-------------|--------|
| Accuracy    | 0.835 |
| Precision (Diabetic) | 0.73 |
| Recall (Diabetic) | 0.78 |
| F1-Score (Diabetic) | 0.75 |

**Confusion Matrix**

```
[[484 78]
 [59 210]]
```


- 484 true negatives (non-diabetic correctly predicted)  
- 210 true positives (diabetic correctly predicted)  
- 78 false positives and 59 false negatives  

The model performs strongly across both classes, slightly favoring non-diabetic predictions due to class imbalance.

---

## ğŸ“‚ Folder Structure

```
knn-with-numpy/
â”œâ”€â”€ diabetes.csv # Dataset (from Kaggle)
â”œâ”€â”€ knn-with-numpy.ipynb # Jupyter Notebook
â””â”€â”€ README.md # This file
```

---

## ğŸš€ How to Run

1. Clone the repository:
   ```bash
   git clone https://github.com/erickhangati/machine-learning-projects.git
   cd machine-learning-projects/KNN/knn-with-numpy
   ```
2. Install dependencies:
   ```bash
   pip install pandas numpy matplotlib seaborn scikit-learn
   ```
3. Install dependencies:
   ```bash
   jupyter notebook knn-with-numpy.ipynb
   ```
 
## âœï¸ What I Learned

- How to manually implement KNN using NumPy without scikit-learn.
- How Euclidean distance drives similarity-based learning.
- The importance of data scaling in distance-based algorithms.
- How to analyze feature relationships and validate statistical significance.
- How to evaluate classification performance using multiple metrics.

## ğŸ“˜ Blog Article

A full, beginner-friendly walkthrough of this project is available on my blog:
[KNN with NumPy: A Beginnerâ€™s Guide to K â€“ Nearest Neighbors](https://erickhangati.com/knn-with-numpy-beginners-guide-k-nearest-neighbor/)